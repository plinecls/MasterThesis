{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab6a2f5",
   "metadata": {},
   "source": [
    "Claes Pauline. Master Digital Text Analysis. Student ID: 20163274\n",
    "\n",
    "# Script for handling all EPUB data (coming from Google Books corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a87f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf45cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bbb6a",
   "metadata": {},
   "source": [
    "# Step 1. Epub to txt\n",
    "In Epub files, there are multiple folders, including one 'content'. This then includes multiple xml files, all containing a piece of text of the work in question.\n",
    "\n",
    "Therefore, we open all xml files in each of the folders, and write them together to 1 txt file that can be queried and edited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570379c",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_txt(path_to_epub, new_path_to_txt): \n",
    "    '''Function to write all of the separate xml files of the EPUB format into one txt file.\n",
    "    Takes as argument a path to the folder containing the epub files, as well as a path to a new folder, in which the \n",
    "    merged .txt files should be placed. Prints its progress. \n",
    "    '''\n",
    "    folders = os.listdir(path_to_epub) # check all files in directory\n",
    "    for item in folders: # iterate over folders\n",
    "        if \".DS_Store\" in folders: \n",
    "            folders.remove(\".DS_Store\") # remove this extension\n",
    "    #folders.remove(\".DS_store\")\n",
    "    \n",
    "    for folder in folders: # iterate over all folders\n",
    "        text_list = [] # create empty list \n",
    "        \n",
    "        for file in glob.glob(f\"{path_to_epub}/{folder}/*.xml\"): # iterate over all xml files\n",
    "            with open(file) as f: \n",
    "                text = f.read() #read in XML data\n",
    "                \n",
    "            text_list.append(text) # append text to list\n",
    "        \n",
    "        # first write a txt file in original folder\n",
    "        with open(f\"{path_to_epub}/{folder}/{folder}.txt\", \"w\") as t: \n",
    "            for item in text_list: \n",
    "                t.write(item) # write all of the elements in the list to one and the same file\n",
    "        \n",
    "        print(f\"-----Done writing {folder}.txt in original folder-----\")\n",
    "        \n",
    "        # then write same file to one folder containing all txts\n",
    "        with open(f\"{new_path_to_txt}/{folder}.txt\", \"w\") as t: \n",
    "            for item in text_list: \n",
    "                t.write(item)\n",
    "        print(f\"-----Done writing {folder}.txt in new txt folder-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe3a1a",
   "metadata": {},
   "source": [
    "### First selection epub to 1 txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb1287",
   "metadata": {},
   "source": [
    "#### early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/paulineclaes/Documents/dta/Thesis/Data/XML/EarlyTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_epub = \"/Users/paulineclaes/Documents/dta/Thesis/Data/XML/EarlyTexts/EarlyTexts_Epub\"\n",
    "new_path_to_txt = \"/Users/paulineclaes/Documents/dta/Thesis/Data/XML/EarlyTexts/EarlyTexts_txt\"\n",
    "get_one_txt(path_to_epub, new_path_to_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5766e6",
   "metadata": {},
   "source": [
    "#### later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_epub = \"/Users/paulineclaes/Documents/dta/Thesis/Data/XML/KandidaatTeksten/KandidaatTeksten_Epub\"\n",
    "new_path_to_txt = \"/Users/paulineclaes/Documents/dta/Thesis/Data/XML/KandidaatTeksten/KandidaatTeksten_txt\"\n",
    "\n",
    "get_one_txt(path_to_epub, new_path_to_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc4985",
   "metadata": {},
   "source": [
    "### Second selection epub to 1 txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbd962",
   "metadata": {},
   "source": [
    "#### early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91200902",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_epub = \"/Users/paulineclaes/Documents/dta/thesis/Data/XML/EarlyTexts/EarlyTexts_SecondSelection/EarlyTexts_SecondSelection_Epub\"\n",
    "new_path_to_txt = \"/Users/paulineclaes/Documents/dta/thesis/Data/XML/EarlyTexts/EarlyTexts_SecondSelection/EarlyTexts_SecondSelection_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_one_txt(path_to_epub, new_path_to_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b49eb",
   "metadata": {},
   "source": [
    "#### later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dff3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_epub = \"/Users/paulineclaes/Documents/dta/thesis/Data/XML/LaterTexts/LaterTexts_SecondSelection/LaterTexts_SecondSelection_Epub\"\n",
    "new_path_to_txt = \"/Users/paulineclaes/Documents/dta/thesis/Data/XML/LaterTexts/LaterTexts_SecondSelection/LaterTexts_SecondSelection_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_one_txt(path_to_epub, new_path_to_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4d0de-9a7f-4413-a45a-baeab41636fb",
   "metadata": {},
   "source": [
    "# Step 2. Epub corrector\n",
    "\n",
    "In the previous step, all XML files from the EPUB files were extracted and written to one file. These EPUB files contain many separate xml files. These were read by the script, and merged into one txt document. In this current step, the merged XML files (merged into a txt file, which still contain a lot of junk) will be cleaned, so only clean text remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0678099b-9b8c-477a-b816-a4fe66b85143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove tags\n",
    "def remove_tags(doc):\n",
    "    '''Function to remove all tags, so only clean text within the tags\n",
    "    remains.'''\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "  \n",
    "    # return data by retrieving the tag content\n",
    "    text = ' '.join(soup.stripped_strings)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_to_txt(path_to_cleaned_txt): \n",
    "    \"\"\"Function that takes as argument the path to the directory containing\n",
    "    tagged, unclean txt files. Then removes the tags using BeautifulSoup, \n",
    "    and writes the clean text to a new txt file.\"\"\"\n",
    "\n",
    "    for file in glob.glob(\"*.txt\"):\n",
    "\n",
    "        with open(file) as f: \n",
    "            text = f.read()\n",
    "\n",
    "        clean_text = remove_tags(text)\n",
    "        \n",
    "        filename = file.replace(\".txt\", r\"\")\n",
    "\n",
    "        with open(f\"{path_to_cleaned_txt}/{filename}_cleaned.txt\", 'w') as t: \n",
    "            t.write(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791a563-66e1-4b34-8b84-11455efeebb9",
   "metadata": {},
   "source": [
    "# Step 3. EPUB parsing\n",
    "\n",
    "In this script, I will parse the texts and make a data frame identical to the one of the texts coming from Frantext. More specifically, I will manipulate the data in such a way that it contains a row per word in the entire corpus, and four columns: word, lemma, POS, and filename. In the very last step, this data frame will be used to query the data for concordances of ALLER + INF. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e829db9-ccd8-4e3e-aec3-70d06adad3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "#from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "#from spacy.language import Language\n",
    "#from spacy.morphology import Morphology\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "#nlp.add_pipe(\"melt_tagger\", after=\"parser\")\n",
    "#nlp.add_pipe(\"french_lemmatizer\", after=\"melt_tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3b005b-6d70-4447-968f-8edb91da5f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VerbForm=Inf\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"aller\")\n",
    "\n",
    "token = doc[0]\n",
    "\n",
    "print(token.morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c438df1-af72-46e7-a02c-50ac4075aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 5500000 # set max length to a random high number to avoid length errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae6e5ae-8a6e-4537-a983-280547e302b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lemma_POS_df(extension):\n",
    "    \"\"\"\n",
    "    Function to get a word-lemma-POS data frame in the same form as the data frame constructed for Frantext data. \n",
    "    Contains four columns: file name, word (as it appears in corpus), lemma (lemma of word), POS (part-of-speech tag)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [] # initiate empty list\n",
    "    \n",
    "    \n",
    "    \n",
    "    for file in glob.glob(extension): # iterate over foldeR\n",
    "        with open(file, 'r') as f: # open each file\n",
    "            text = f.read() # read it in\n",
    "        \n",
    "        filename = file.replace(\".txt\", \"\") # get the file name without the extension\n",
    "        \n",
    "        df = pd.DataFrame({ # construct data frame for each separate text. This is done so texts will not get mixed\n",
    "          #  \"filename\": [filename], \n",
    "            \"word\" : [token for token in nlp(text)], # word as it appears in corpus (tokenized)\n",
    "            \"lemma\" : [token.lemma_ for token in nlp(text)], # lemma of that word\n",
    "            \"POS\" : [token.pos_ for token in nlp(text)] # part of speech tag of that word\n",
    "        })\n",
    "        \n",
    "        df[\"filename\"] = filename # add file name\n",
    "        \n",
    "        data.append(df) # add dataframe per file to list\n",
    "        \n",
    "    word_lemma_POS_df = pd.concat(data, axis=\"rows\", ignore_index=True) # concatenate all data frames in list to one dataframe\n",
    "    \n",
    "    return word_lemma_POS_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec46d6-a0e1-4f1e-8e22-d185b3f201b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 4. EPUB concordancer\n",
    "\n",
    "After having made a WLP (Word-Lemma-POS) dataframe for the EPUB files, this WLP-dataframe will now be queried for occurrences of 'aller + INF'. When found, the previous 50 words will be joined and the following 50 words will be joined, in order to obtain a concordance dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946f3c0b-bfbb-4537-a74c-454d6a8ce415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordancer_to_df(df):\n",
    "    \"\"\"\n",
    "        \n",
    "        This function takes as arguments the dataframe constructed from the Epub XML.\n",
    "        \n",
    "        1. Subsequently, it builds a list of indices that answer the following criteria: the lemma must be 'aller' and it must be followed by an infinitive.\n",
    "        \n",
    "        2. Based on this index list, which thus only consists of indices that have aller + inf, it builds a dataframe for each index in the index list: \n",
    "            a) a column for the filename, based on the file name in the original dataframe\n",
    "            b) the previous 50 words preceding the instance of 'aller' that is followed by an infinitive\n",
    "            c) the word immediately preceding 'aller'\n",
    "            d) the instance of 'aller' itself\n",
    "            e) the parts-of-speech tag of the instance of 'aller'\n",
    "            f) the infinitive following 'aller'\n",
    "            g) the word immediately following the infinitive\n",
    "            h) the parts-of-speech tag of the word immediately following the infinitive\n",
    "            i) the following 50 words after the infinitive\n",
    "        \n",
    "        3. So, after having constructed a dataframe for each index in the index list, it adds these dataframes to the empty list 'data', and subsequently concatenates \n",
    "        them into one dataframe.\n",
    "        \n",
    "    \"\"\"\n",
    "    index_list = [index for index in list(df[df['lemma'] == \"aller\"].index) if df[\"POS\"].iloc[index+1] == \"VERB\"] \n",
    "            # build index list for each row that has a lemma 'aller' and for which the following POS-tag is 'VERB'\n",
    "            # this way, we get the indices of the concordances that we are looking for\n",
    "    \n",
    "    data = [] # instantiate empty list\n",
    "    \n",
    "    for index in index_list: # build dataframe for each index in the index list\n",
    "        temp = pd.DataFrame({\n",
    "            \"filename\" : [df[\"filename\"][index]], # get file name\n",
    "            \"previous50\": [\" \".join(df[\"word\"][index-50:index-1])], # get previous 50 words\n",
    "            \"prev1\": [df[\"word\"].iloc[index-1]], # get word right before\n",
    "            \"aller\": [df[\"word\"].iloc[index]], # get instance of aller itself (not lemmatized)\n",
    "            \"aller_POS\": [df[\"POS\"].iloc[index]], # get POS-tag of ALLER (this will be 'VERB', but this is just to check if \n",
    "                                                  # everything worked correctly afterward)\n",
    "            \"INF\": [df[\"word\"].iloc[index+1]], # get verb right after 'aller' (not lemmatized)\n",
    "            \"next1\": [df[\"word\"].iloc[index+2]], # get word right after verb following 'aller'\n",
    "            \"next1_POS\" : [df[\"POS\"].iloc[index+2]], # get pos-tag of word right after verb following 'aller'\n",
    "            \"next50\": [\" \".join(df[\"word\"][index+3:index+50])] # get following 50 words\n",
    "        })\n",
    "        \n",
    "        data.append(temp) # add dataframes to list\n",
    "        \n",
    "    concordance_df = pd.concat(data, axis=\"rows\", ignore_index=True) # concatenate\n",
    "    \n",
    "    return concordance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041d657-f457-423a-a0f2-cf905181e6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
