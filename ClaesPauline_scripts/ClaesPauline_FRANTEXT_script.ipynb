{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "steady-parcel",
   "metadata": {},
   "source": [
    "Claes Pauline. Master Digital Text Analysis. Student ID: 20163274\n",
    "\n",
    "# Script for handling FRANTEXT XML DATA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba13914-9041-4c1e-9ecd-9727a9d7449a",
   "metadata": {},
   "source": [
    "# Step 1. Correct XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34e2b61b-6514-4744-8f39-f2ca379ad9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob, os\n",
    "original_stdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f865c76-8111-4b28-95a3-b07e5809419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XML_corrector(source_file_path):\n",
    "    '''Function to insert line break after each closing tag, to make \n",
    "    the XML documents a bit more readable.'''\n",
    "    with open(source_file_path, 'r') as f: \n",
    "        data = f.read()\n",
    "    print(len(data))\n",
    "    \n",
    "    corrected = data.replace(r'/>', '/>'+'\\n') # insert line break after each closing tag\n",
    "    print(corrected[:200])\n",
    "    #return corrected\n",
    "\n",
    "def write_corr_XML(source_file_path, folder_name):\n",
    "    '''Function to make XML more easily queryable.'''\n",
    "    \n",
    "    file_name = source_file_path.replace('.xml', r'') # get file name\n",
    "\n",
    "    with open(source_file_path, 'r') as f: \n",
    "        data = f.read() # read in data\n",
    "    \n",
    "    \n",
    "    \n",
    "    corrected = data.replace(r'>', '>\\n') # enter line break after each closing tag\n",
    "    corrected = corrected.replace('x:wf', 'line') # replace tag 'x:wf' with 'line', as the LXML module used later on was not\n",
    "                                                  # able to query the original 'x:wf' tag\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(str(folder_name) + \"temp_\" + file_name + '.xml', 'w') as f: # write adapted files to new XML documents\n",
    "        sys.stdout = f # Change the standard output to the file we created.\n",
    "        print(corrected)\n",
    "        sys.stdout = original_stdout\n",
    "    \n",
    "   # print(file_name, len(data))\n",
    "    \n",
    "    print(f\"\"\" {'-'*10} {file_name} {'-'*10} \\n *Original XML length:{len(data)} \\n *Corrected XML length: {len(corrected)}\"\"\") # obtain text length\n",
    "    \n",
    "   #print(corrected[:200])\n",
    "    #return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "balanced-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------- FONTENELLE_Entretiens_Pluralite_des_mondes ---------- \n",
      " *Original XML length:1547129 \n",
      " *Corrected XML length: 1588252\n",
      " ---------- VOITURE_VINCENT_Lettres ---------- \n",
      " *Original XML length:6809593 \n",
      " *Corrected XML length: 6985423\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE EXECUTION OF CODE\n",
    "for file in glob.glob(\"*.xml\"):\n",
    "    write_corr_XML(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdadea1-781c-4087-8d97-7d30a86fd6ba",
   "metadata": {},
   "source": [
    "# Step 2. Frantext XML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095b5904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "import xml.etree.ElementTree as et "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f806f6-9e97-4816-8802-cfda21d2557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_list = [\"file_name\", \"word\", \"lemma\", \"POS\"]\n",
    "queried_tag = '{http://www.tei-c.org/ns/1.0}line' # this is the tag that I changed\n",
    "                                                  # from 'x:wf' to 'line' earlier\n",
    "\n",
    "\n",
    "def write_attr_df(path, col_list, queried_tag):\n",
    "    '''Read in XML data, search for \"lemma\", \"pos\", and \"word\" tags, \n",
    "    and construct data frame with one row per word, and four columns:\n",
    "    file name, word, lemma, POS. \n",
    "    \n",
    "    This results in a data frame that is easy to query and identical to \n",
    "    the one used for the EPUB data.'''\n",
    "    \n",
    "    xtree = et.parse(path) # parse the XML using etree module\n",
    "    xroot = xtree.getroot() # get root\n",
    "    rows = [] # instantiate empty list\n",
    "    file_name = path.replace('.xml', r'')\n",
    "    \n",
    "    for child in xroot.iter(): # iterate over root\n",
    "        if child.tag == queried_tag: # if one of the child tags is the one that we're looking for\n",
    "            lemma = child.attrib.get('lemma') # get the attribute 'lemma'\n",
    "            pos = child.attrib.get('pos') # get the attribute 'pos' (pos-tag)\n",
    "            word = child.attrib.get('word') # get the attribute 'word'\n",
    "            \n",
    "            rows.append({\"file_name\": file_name, \"lemma\": lemma, \"POS\": pos, \"word\":word})\n",
    "            # append dictionary containing file name, lemma, pos, word\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns = col_list)\n",
    "    \n",
    "    return df\n",
    "  #  print(f\"writing {file_name} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ba3c2-6eef-4dd5-bb37-c9ab6d270df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXAMPLE EXECUTION OF CODE:\n",
    "import glob, os\n",
    "\n",
    "df_list = [] # make empty list to add data frames to \n",
    "\n",
    "for file in glob.glob(\"*.xml\"):\n",
    "    df = write_attr_df(file, col_list, queried_tag)\n",
    "    \n",
    "    df_list.append(df)\n",
    "    \n",
    "df_result = pd.concat(df_list, axis = 0)\n",
    "\n",
    "df_result.to_csv(\"frantext_XML.csv\", sep=\",\", header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ef5c501c-81d6-4e5c-a952-b699cf5df6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes</td>\n",
       "      <td>DÉDICACE</td>\n",
       "      <td>dédicace</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes</td>\n",
       "      <td>à</td>\n",
       "      <td>à</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes</td>\n",
       "      <td>Monsieur</td>\n",
       "      <td>Monsieur</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes</td>\n",
       "      <td>L.</td>\n",
       "      <td>L.</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes</td>\n",
       "      <td>vous</td>\n",
       "      <td>vous</td>\n",
       "      <td>CLS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name      word     lemma  POS\n",
       "0  WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes  DÉDICACE  dédicace   NC\n",
       "1  WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes         à         à    P\n",
       "2  WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes  Monsieur  Monsieur   NC\n",
       "3  WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes        L.        L.   NP\n",
       "4  WFtemp_FONTENELLE_Entretiens_Pluralite_des_mondes      vous      vous  CLS"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"frantext_XML.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869641c-c9ec-49ad-935b-53428f608afe",
   "metadata": {},
   "source": [
    "# Step 3. Frantext concordancer\n",
    "\n",
    "I will now use the data frame constructed above to construct a data frame consisting of only concordances of ALLER + INF. This is the same function as used for the EPUB data. When 'aller + INF' found, the previous 50 words will be joined and the following 50 words will be joined, in order to retrieve a concordance dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e8359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frantext_concordancer_to_df(df):\n",
    "    \"\"\"\n",
    "        \n",
    "        This function takes as arguments the dataframe constructed from the Frantext XML.\n",
    "        \n",
    "        1. Subsequently, it builds a list of indices that answer the following criteria: the lemma must be 'aller' and it must be followed by an infinitive.\n",
    "        \n",
    "        2. Based on this index list, which thus only consists of indices that have aller + inf, it builds a dataframe for each index in the index list: \n",
    "            a) a column for the filename, based on the file name in the original dataframe\n",
    "            b) the previous 50 words preceding the instance of 'aller' that is followed by an infinitive\n",
    "            c) the word immediately preceding 'aller'\n",
    "            d) the instance of 'aller' itself\n",
    "            e) the parts-of-speech tag of the instance of 'aller'\n",
    "            f) the infinitive following 'aller'\n",
    "            g) the word immediately following the infinitive\n",
    "            h) the parts-of-speech tag of the word immediately following the infinitive\n",
    "            i) the following 50 words after the infinitive\n",
    "        \n",
    "        3. So, after having constructed a dataframe for each index in the index list, it adds these dataframes to the empty list 'data', and subsequently concatenates \n",
    "        them into one dataframe.\n",
    "        \n",
    "    \"\"\"\n",
    "    index_list = [index for index in list(df[df['lemma'] == \"aller\"].index) if df[\"POS\"].iloc[index+1] == \"VINF\"] # build index list\n",
    "                # this list comprehension builds a list of indices of rows where the lemma is aller, and the following\n",
    "                # POS-tag is an infinitive \n",
    "        \n",
    "    data = [] # instantiate empty list\n",
    "    \n",
    "    for index in index_list: # build dataframe for each index in the index list\n",
    "        temp = pd.DataFrame({\n",
    "            \"filename\" : [df[\"file_name\"][index]], # get filename of that index\n",
    "            \"previous50\": [\" \".join(df[\"word\"][index-50:index-1])], # join previous 50 words\n",
    "            \"prev1\": [df[\"word\"].iloc[index-1]], # get previous word\n",
    "            \"aller\": [df[\"word\"].iloc[index]], # get instance of 'aller' itself\n",
    "            \"aller_POS\": [df[\"POS\"].iloc[index]], # get pos-tag of 'aller'\n",
    "            \"INF\": [df[\"word\"].iloc[index+1]], # get infinitive following 'aller'\n",
    "            \"next1\": [df[\"word\"].iloc[index+2]], # get word following infinitive\n",
    "            \"next1_POS\" : [df[\"POS\"].iloc[index+2]], # get POS-tag of word following infinitive\n",
    "            \"next50\": [\" \".join(df[\"word\"][index+3:index+50])] # join following 50 words\n",
    "        })\n",
    "        \n",
    "        data.append(temp) # add dataframes to list\n",
    "        \n",
    "    concordance_df = pd.concat(data, axis=\"rows\", ignore_index=True) # concatenate all dataframes in list\n",
    "    \n",
    "    return concordance_df # return one big data frame containing all the concordances \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ce23dfb-94e2-4213-8a0f-18bc1d3dd2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>previous50</th>\n",
       "      <th>prev1</th>\n",
       "      <th>aller</th>\n",
       "      <th>aller_POS</th>\n",
       "      <th>INF</th>\n",
       "      <th>next1</th>\n",
       "      <th>next1_POS</th>\n",
       "      <th>next50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WFtemp_LABE_Debat_de_folie_et_amour</td>\n",
       "      <td>à gouverner les Viles , sans que lon l' apelle...</td>\n",
       "      <td>d'</td>\n",
       "      <td>aller</td>\n",
       "      <td>VINF</td>\n",
       "      <td>planter</td>\n",
       "      <td>des</td>\n",
       "      <td>P+D</td>\n",
       "      <td>chous . Le fol ira tant et viendra , en donner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WFtemp_LABE_Debat_de_folie_et_amour</td>\n",
       "      <td>ay dit . Quand Mercure ut fini la defense de F...</td>\n",
       "      <td>,</td>\n",
       "      <td>và</td>\n",
       "      <td>V</td>\n",
       "      <td>prononcer</td>\n",
       "      <td>un</td>\n",
       "      <td>DET</td>\n",
       "      <td>arrest interlocutoire en cette maniere : Pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WFtemp_Montaigne_Essais1-2</td>\n",
       "      <td>on peut faire aux ennemis en guerre , cela est...</td>\n",
       "      <td>les</td>\n",
       "      <td>alla</td>\n",
       "      <td>V</td>\n",
       "      <td>charger</td>\n",
       "      <td>tous</td>\n",
       "      <td>PRO</td>\n",
       "      <td>endormis et les défict , alleguant qu' en sa t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WFtemp_Montaigne_Essais1-2</td>\n",
       "      <td>meint un , qui pour avoir ou haussé la main , ...</td>\n",
       "      <td>estant</td>\n",
       "      <td>allé</td>\n",
       "      <td>VPP</td>\n",
       "      <td>recognoistre</td>\n",
       "      <td>la</td>\n",
       "      <td>DET</td>\n",
       "      <td>Ville d' Arle , et s' estant jetté hors du cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WFtemp_Montaigne_Essais1-2</td>\n",
       "      <td>tels Princes , que le plus grand soit avant le...</td>\n",
       "      <td>moindres</td>\n",
       "      <td>vont</td>\n",
       "      <td>V</td>\n",
       "      <td>trouver</td>\n",
       "      <td>,</td>\n",
       "      <td>PONCT</td>\n",
       "      <td>et le recherchent , non pas luy eux . Non seul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  \\\n",
       "0  WFtemp_LABE_Debat_de_folie_et_amour   \n",
       "1  WFtemp_LABE_Debat_de_folie_et_amour   \n",
       "2           WFtemp_Montaigne_Essais1-2   \n",
       "3           WFtemp_Montaigne_Essais1-2   \n",
       "4           WFtemp_Montaigne_Essais1-2   \n",
       "\n",
       "                                          previous50     prev1  aller  \\\n",
       "0  à gouverner les Viles , sans que lon l' apelle...        d'  aller   \n",
       "1  ay dit . Quand Mercure ut fini la defense de F...         ,     và   \n",
       "2  on peut faire aux ennemis en guerre , cela est...       les   alla   \n",
       "3  meint un , qui pour avoir ou haussé la main , ...    estant   allé   \n",
       "4  tels Princes , que le plus grand soit avant le...  moindres   vont   \n",
       "\n",
       "  aller_POS           INF next1 next1_POS  \\\n",
       "0      VINF       planter   des       P+D   \n",
       "1         V     prononcer    un       DET   \n",
       "2         V       charger  tous       PRO   \n",
       "3       VPP  recognoistre    la       DET   \n",
       "4         V       trouver     ,     PONCT   \n",
       "\n",
       "                                              next50  \n",
       "0  chous . Le fol ira tant et viendra , en donner...  \n",
       "1  arrest interlocutoire en cette maniere : Pour ...  \n",
       "2  endormis et les défict , alleguant qu' en sa t...  \n",
       "3  Ville d' Arle , et s' estant jetté hors du cou...  \n",
       "4  et le recherchent , non pas luy eux . Non seul...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### EXAMPLE\n",
    "df_early.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-terry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
